{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step1_AIë©´ì ‘ê´€ Agent v1.0**"
      ],
      "metadata": {
        "id": "mSB2IiVH8B1v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU-xxYwejwGR"
      },
      "source": [
        "## **1. í™˜ê²½ì¤€ë¹„**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdcBWhy_F_Hm"
      },
      "source": [
        "### (1) êµ¬ê¸€ ë“œë¼ì´ë¸Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë” ìƒì„±\n",
        "* ìƒˆ í´ë”(project_genai)ë¥¼ ìƒì„±í•˜ê³ \n",
        "* ì œê³µ ë°›ì€ íŒŒì¼ì„ ì—…ë¡œë“œ"
      ],
      "metadata": {
        "id": "xUOpvAJGGJnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°"
      ],
      "metadata": {
        "id": "4jUC5td4GLEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tEfLUT6ZGEJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2a5d59-e3ee-4702-c579-457c50c7f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ],
      "metadata": {
        "id": "PepxmQuiGzkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/project_genai/requirements.txt -q"
      ],
      "metadata": {
        "id": "TwO3_Qx4PlM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS5BhycUFUMI"
      },
      "source": [
        "### (3) OpenAI API Key í™•ì¸\n",
        "* api_key.txt íŒŒì¼ì— ë‹¤ìŒì˜ í‚¤ë¥¼ ë“±ë¡í•˜ì„¸ìš”.\n",
        "    * OPENAI_API_KEY\n",
        "    * NGROK_AUTHTOKEN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/project_genai/'\n",
        "# API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "AaZBGfeWNMRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ['OPENAI_API_KEY'][:30])"
      ],
      "metadata": {
        "id": "GqSUhiv8wKxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30880564-ef85-436c-981f-69f708c78e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk-proj-r111drRrWBH3MHbjiUfFop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. App.py**\n",
        "\n",
        "* ì•„ë˜ ì½”ë“œì—, Step1 í˜¹ì€ ê³ ë„í™” ëœ Step2 íŒŒì¼ ì½”ë“œë¥¼ ë¶™ì¸ë‹¤.\n",
        "    * ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "    * í•¨ìˆ˜ë“¤ê³¼ ê·¸ë˜í”„\n",
        "* Gradio ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì¼ë¶€ ìˆ˜ì • ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "ULAOaRHmShq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ---------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import openai\n",
        "import random\n",
        "import ast\n",
        "import fitz\n",
        "from docx import Document\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "from typing import Annotated, Literal, Sequence, TypedDict, List, Dict\n",
        "from langchain import hub\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "## ---------------- 1ë‹¨ê³„ : ì‚¬ì „ì¤€ë¹„ ----------------------\n",
        "\n",
        "# 1) íŒŒì¼ ì…ë ¥ --------------------\n",
        "def extract_text_from_file(file_path: str) -> str:\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == \".pdf\":\n",
        "        doc = fitz.open(file_path)\n",
        "        text = \"\\n\".join(page.get_text() for page in doc)\n",
        "        doc.close()\n",
        "        return text\n",
        "    elif ext == \".docx\":\n",
        "        doc = Document(file_path)\n",
        "        return \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
        "    else:\n",
        "        raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” DOCXë§Œ í—ˆìš©ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "# 2) State ì„ ì–¸ --------------------\n",
        "from typing import TypedDict, List, Dict\n",
        "\n",
        "class ExamState(TypedDict):\n",
        "    # ê³ ì • ì •ë³´\n",
        "    exam_text: str\n",
        "    exam_summary: str\n",
        "    exam_keywords: List[str]\n",
        "    question_strategy: Dict[str, Dict]\n",
        "\n",
        "    # ë¬¸ë‹µ ë¡œê·¸\n",
        "    current_question: str\n",
        "    current_answer: str\n",
        "    current_strategy: str\n",
        "    conversation: List[Dict[str, str]]\n",
        "    evaluation : List[Dict[str, str]]\n",
        "    next_step : str\n",
        "\n",
        "# 3) resume ë¶„ì„ --------------------\n",
        "def analyze_exam(state: ExamState) -> ExamState:\n",
        "    exam_text = state.get(\"exam_text\", \"\")\n",
        "    if not exam_text:\n",
        "        raise ValueError(\"exam_textê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "    # ìš”ì•½ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "    summary_prompt = ChatPromptTemplate.from_template(\n",
        "        '''ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ì‚¬íšŒíƒêµ¬ ê³¼ëª©ì¸ 'ì •ì¹˜ì™€ ë²•' ê¸°ì¶œë¬¸ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ì˜ê³ ì‚¬ë¥¼ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
        "        ë‹¤ìŒ ê¸°ì¶œë¬¸ì œ ë‚´ìš©ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ í•„ìš”í•œ ë‚´ìš©ì„ ë¬¸ì œë³„ë¡œ ëª¨ë‘ ìš”ì•½ í•´ì¤˜. ì´ 20ê°œì˜ ìš”ì•½ì´ ë‚˜ì™€ì•¼í•´(ìš”ì•½ì‹œ ** ê¸°í˜¸ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê²ƒ):\\n\\n{exam_text}'''\n",
        "    )\n",
        "    formatted_summary_prompt = summary_prompt.format(exam_text=exam_text)\n",
        "    summary_response = llm.invoke(formatted_summary_prompt)\n",
        "    exam_summary = summary_response.content.strip()\n",
        "\n",
        "    # í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "    keyword_prompt = ChatPromptTemplate.from_template(\n",
        "        '''ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ì‚¬íšŒíƒêµ¬ ê³¼ëª©ì¸ 'ì •ì¹˜ì™€ ë²•' ê¸°ì¶œë¬¸ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ì˜ê³ ì‚¬ë¥¼ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
        "        ë‹¤ìŒ ê¸°ì¶œë¬¸ì œ ë‚´ìš©ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë¬¸ì œë‹¹ 1ê°œì”© ì´ 20ê°œ ì¶”ì¶œí•´ì¤˜.\n",
        "        í‚¤ì›Œë“œëŠ” ê·¸ ë¬¸ì œì—ì„œ ë¬¼ì–´ë³´ëŠ” ê³ ë“±í•™êµ ìˆ˜ëŠ¥ ê°œë…ì„ ì¶”ì¶œí•´ì¤˜.\n",
        "        ë„ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì¤˜:\\n\\n{exam_text}'''\n",
        "    )\n",
        "    formatted_keyword_prompt = keyword_prompt.format(exam_text=exam_text)\n",
        "    keyword_response = llm.invoke(formatted_keyword_prompt)\n",
        "\n",
        "    parser = CommaSeparatedListOutputParser()\n",
        "    exam_keywords = parser.parse(keyword_response.content)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"exam_summary\": exam_summary,\n",
        "        \"exam_keywords\": exam_keywords,\n",
        "    }\n",
        "\n",
        "# 4) ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½ --------------------\n",
        "import json\n",
        "\n",
        "def generate_question_strategy(state: ExamState) -> ExamState:\n",
        "    exam_summary = state.get(\"exam_summary\", \"\")\n",
        "    exam_keywords = \", \".join(state.get(\"exam_keywords\", []))\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ì‚¬íšŒíƒêµ¬ ê³¼ëª©ì¸ 'ì •ì¹˜ì™€ ë²•' ê¸°ì¶œë¬¸ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ **ìˆ˜ëŠ¥í˜• ì‚¬ë¡€ ê¸°ë°˜ 5ì§€ì„ ë‹¤í˜• ëª¨ì˜ê³ ì‚¬**ë¥¼ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
        "\n",
        "    - ê¸°ì¶œë¬¸ì œ ìš”ì•½:\n",
        "    {exam_summary}\n",
        "\n",
        "    - ê¸°ì¶œë¬¸ì œ í‚¤ì›Œë“œ:\n",
        "    {exam_keywords}\n",
        "\n",
        "      ì¶œì œ ì¡°ê±´:\n",
        "    - ë¬¸ì œëŠ” ë°˜ë“œì‹œ 2025í•™ë…„ë„ ê¸°ì¤€ ê³ ë“±í•™êµ 'ì •ì¹˜ì™€ ë²•' êµê³¼ì„œ ë° ìˆ˜ëŠ¥Â·ëª¨ì˜ê³ ì‚¬ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¶œì œí•˜ì„¸ìš”.\n",
        "    - ë¬¸ì œì˜ í˜•ì‹, ì„ íƒì§€ êµ¬ì„±ê³¼ ë¬¸ì œë¥¼ í‘¸ëŠ” ë° í•„ìš”í•œ ê°œë…ì€ ë‹¤ìŒì„ ì°¸ê³ í•˜ë˜, ì‚¬ë¡€ëŠ” ìƒˆë¡­ê²Œ êµ¬ì„±í•˜ì„¸ìš”.\n",
        "    - ê° ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ì—¬ì•¼ í•©ë‹ˆë‹¤:\n",
        "  {{\n",
        "    \"ë¬¸ì œë²ˆí˜¸\": 1,\n",
        "    \"ë¬¸ì œ\": \"ë‹¤ìŒ ì‚¬ë¡€ì— ëŒ€í•œ ë²•ì  íŒë‹¨ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?\",\n",
        "    \"ì‚¬ë¡€\": \"ê°‘ì€ ì„ê³¼ í˜¼ì¸ í›„ Aë¥¼ ë‚³ê³  ì‚´ë‹¤ê°€ ìƒí™œí•„ìˆ˜í’ˆ êµ¬ë§¤ë¥¼ ìœ„í•´ ì¹œêµ¬ ë³‘ìœ¼ë¡œë¶€í„° ì—¬ëŸ¬ ì°¨ë¡€ ëˆì„ ë¹Œë ¤ ì‚¬ìš©í•  ë§Œí¼ ê²½ì œì  ë¬¸ì œê°€ ì§€ì†ë˜ì ê²°êµ­ ì´í˜¼ ìˆ™ë ¤ ê¸°ê°„ì„ ê±°ì³ ì´í˜¼í•˜ì˜€ë‹¤. í•œí¸ ì •ê³¼ ë¬´ëŠ” Bì™€ Cë¥¼ ë‚³ê³  ì‚´ì•˜ìœ¼ë‚˜ ë¬´ì˜ ë¶€ì •í•œ í–‰ìœ„ë¡œ ì´í˜¼í•˜ì˜€ìœ¼ë©° BëŠ” ì •ê³¼, CëŠ” ë¬´ì™€ ì‚´ê¸°ë¡œ ê²°ì •í•˜ì˜€ë‹¤. ëª‡ ë…„ ë’¤, Aë¥¼ í™€ë¡œ ì–‘ìœ¡í•˜ë˜ ê°‘ì€ ì •ê³¼ í˜¼ì¸í•˜ì˜€ìœ¼ë©° ì •ì€ Aë¥¼ ì¹œì–‘ìë¡œ ì…ì–‘í•˜ì˜€ë‹¤. ì´í›„ ë¬´ê°€ ê°‘ì‘ìŠ¤ëŸ° ì‚¬ê³ ë¡œ ì‚¬ë§í•˜ë©´ì„œ ê°‘ì€ Bì™€ Cë¥¼ ì¹œì–‘ìë¡œ ì…ì–‘í•˜ì˜€ë‹¤.\"\",\n",
        "    \"ì„ íƒì§€\": [\"â‘  ê°‘ê³¼ì˜ í˜¼ì¸ ê¸°ê°„ ë™ì•ˆ ì„ì€ ê°‘ì´ ë³‘ì—ê²Œ ë¹Œë¦° ìƒí™œí•„ìˆ˜í’ˆ êµ¬ë§¤ ë¹„ìš©ì„ ê°šì„ ì˜ë¬´ê°€ ì—†ë‹¤.\",\n",
        "     \"â‘¡ ê°‘ê³¼ ì„ì˜ ì´í˜¼ì€ ê°€ì • ë²•ì›ì—ì„œ ì´í˜¼ ì˜ì‚¬ë¥¼ í™•ì¸ë°›ëŠ” ì¦‰ì‹œ ê·¸ íš¨ë ¥ì´ ë°œìƒí•œë‹¤.\",\n",
        "      \"â‘¢ ë¬´ëŠ” ì •ê³¼ì˜ ì´í˜¼ ì‹œ í˜¼ì¸ ì¤‘ ê³µë™ìœ¼ë¡œ ë§ˆë ¨í•œ ì¬ì‚°ì— ëŒ€í•´ ì¬ì‚° ë¶„í• ì„ ì²­êµ¬í•  ìˆ˜ ì—†ë‹¤.\",\n",
        "       \"â‘£ ì •ì´ Aë¥¼ ì…ì–‘í•¨ì— ë”°ë¼ ì„ê³¼ Aì˜ ì¹œì ê´€ê³„ëŠ” ì¢…ë£Œëœë‹¤.\",\n",
        "        \"â‘¤ ì…ì–‘ìœ¼ë¡œ ì¸í•´ B, C ëª¨ë‘ ê°‘ê³¼ ì •ì˜ í˜¼ì¸ ì™¸ ì¶œìƒìë¡œ ê°„ì£¼ëœë‹¤.\"],\n",
        "    \"ì •ë‹µ\": \"â‘£\"\n",
        "  }}\n",
        "        \"ì‚¬ë¡€\" : \"\n",
        "    - ì´ 20ê°œì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³ , ì¶œë ¥ì€ ë¬¸ì œ ë¦¬ìŠ¤íŠ¸ë§Œ í¬í•¨ëœ JSON ë°°ì—´ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "       ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ (JSON ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥):\n",
        "\n",
        "    [\n",
        "      {{\n",
        "        \"ë¬¸ì œ1\": \"ë‹¤ìŒ ì‚¬ë¡€ì— ëŒ€í•œ ë²•ì  íŒë‹¨ìœ¼ë¡œ ì˜³ì€ ê²ƒì€?\"\n",
        "        \"ì‚¬ë¡€\" : \"ê°‘ì€ ì„ê³¼ í˜¼ì¸ í›„ Aë¥¼ ë‚³ê³  ì‚´ë‹¤ê°€ ìƒí™œí•„ìˆ˜í’ˆ êµ¬ë§¤ë¥¼ ìœ„í•´ ì¹œêµ¬ ë³‘ìœ¼ë¡œë¶€í„° ì—¬ëŸ¬ ì°¨ë¡€ ëˆì„ ë¹Œë ¤ ì‚¬ìš©í•  ë§Œí¼ ê²½ì œì  ë¬¸ì œê°€ ì§€ì†ë˜ì ê²°êµ­ ì´í˜¼ ìˆ™ë ¤ ê¸°ê°„ì„ ê±°ì³ ì´í˜¼í•˜ì˜€ë‹¤. í•œí¸ ì •ê³¼ ë¬´ëŠ” Bì™€ Cë¥¼ ë‚³ê³  ì‚´ì•˜ìœ¼ë‚˜ ë¬´ì˜ ë¶€ì •í•œ í–‰ìœ„ë¡œ ì´í˜¼í•˜ì˜€ìœ¼ë©° BëŠ” ì •ê³¼, CëŠ” ë¬´ì™€ ì‚´ê¸°ë¡œ ê²°ì •í•˜ì˜€ë‹¤. ëª‡ ë…„ ë’¤, Aë¥¼ í™€ë¡œ ì–‘ìœ¡í•˜ë˜ ê°‘ì€ ì •ê³¼ í˜¼ì¸í•˜ì˜€ìœ¼ë©° ì •ì€ Aë¥¼ ì¹œì–‘ìë¡œ ì…ì–‘í•˜ì˜€ë‹¤. ì´í›„ ë¬´ê°€ ê°‘ì‘ìŠ¤ëŸ° ì‚¬ê³ ë¡œ ì‚¬ë§í•˜ë©´ì„œ ê°‘ì€ Bì™€ Cë¥¼ ì¹œì–‘ìë¡œ ì…ì–‘í•˜ì˜€ë‹¤.\"\n",
        "        \"ì„ íƒì§€\": [\n",
        "          \"â‘  ê°‘ê³¼ì˜ í˜¼ì¸ ê¸°ê°„ ë™ì•ˆ ì„ì€ ê°‘ì´ ë³‘ì—ê²Œ ë¹Œë¦° ìƒí™œí•„ìˆ˜í’ˆ êµ¬ë§¤ ë¹„ìš©ì„ ê°šì„ ì˜ë¬´ê°€ ì—†ë‹¤.\",\n",
        "          \"â‘¡ ê°‘ê³¼ ì„ì˜ ì´í˜¼ì€ ê°€ì • ë²•ì›ì—ì„œ ì´í˜¼ ì˜ì‚¬ë¥¼ í™•ì¸ë°›ëŠ” ì¦‰ì‹œ ê·¸ íš¨ë ¥ì´ ë°œìƒí•œë‹¤.\",\n",
        "          \"â‘¢ ë¬´ëŠ” ì •ê³¼ì˜ ì´í˜¼ ì‹œ í˜¼ì¸ ì¤‘ ê³µë™ìœ¼ë¡œ ë§ˆë ¨í•œ ì¬ì‚°ì— ëŒ€í•´ ì¬ì‚° ë¶„í• ì„ ì²­êµ¬í•  ìˆ˜ ì—†ë‹¤.\",\n",
        "          \"â‘£ ì •ì´ Aë¥¼ ì…ì–‘í•¨ì— ë”°ë¼ ì„ê³¼ Aì˜ ì¹œì ê´€ê³„ëŠ” ì¢…ë£Œëœë‹¤.\",\n",
        "          \"â‘¤ ì…ì–‘ìœ¼ë¡œ ì¸í•´ B, C ëª¨ë‘ ê°‘ê³¼ ì •ì˜ í˜¼ì¸ ì™¸ ì¶œìƒìë¡œ ê°„ì£¼ëœë‹¤.\"\n",
        "        ],\n",
        "        \"ì •ë‹µ\": \"â‘£\"\n",
        "      }},\n",
        "      ...\n",
        "    ]\n",
        "\n",
        "    â— ì¶œë ¥ì€ ì½”ë“œë¸”ëŸ­ ì—†ì´ ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ë¬¸ìì—´ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    â— ëª¨ë“  í‚¤ì™€ ê°’ì€ í°ë”°ì˜´í‘œ(\")ë¡œ ê°ì‹¸ì•¼ í•˜ë©°, ì •ë‹µì€ ë°˜ë“œì‹œ \"â‘ \", \"â‘¡\" ë“±ì˜ ë¬¸ìì—´ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.\n",
        "    \"\"\")\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    formatted_prompt = prompt.format(\n",
        "        exam_summary=exam_summary,\n",
        "        exam_keywords=exam_keywords\n",
        "    )\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    raw_output = response.content.strip()\n",
        "\n",
        "    # ì½”ë“œë¸”ëŸ­ ì œê±°\n",
        "    if \"```\" in raw_output:\n",
        "        import re\n",
        "        raw_output = re.sub(r\"```(?:json)?\", \"\", raw_output).replace(\"```\", \"\").strip()\n",
        "\n",
        "    # JSON íŒŒì‹±\n",
        "    try:\n",
        "        question_list = json.loads(raw_output)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(\"ë¬¸ì œë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\\nì›ë³¸:\\n\" + raw_output) from e\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"question_strategy\": question_list  # ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì €ì¥\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# 5) 1ë‹¨ê³„ í•˜ë‚˜ë¡œ ë¬¶ê¸° --------------------\n",
        "\n",
        "def preProcessing_Exam(file_path: str) -> ExamState:\n",
        "    # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "    exam_text = extract_text_from_file(file_path)\n",
        "\n",
        "    # state ì´ˆê¸°í™”\n",
        "    initial_state: ExamState = {\n",
        "        \"exam_text\": exam_text,\n",
        "        \"exam_summary\": '',\n",
        "        \"exam_keywords\": [],\n",
        "        \"question_strategy\": [],\n",
        "        \"current_question\": '',\n",
        "        \"current_answer\": '',\n",
        "        \"current_strategy\": '',\n",
        "        \"conversation\": [],\n",
        "        \"evaluation\": [],\n",
        "        \"next_step\": ''\n",
        "    }\n",
        "\n",
        "    # exam ë¶„ì„ â†’ ìš”ì•½ ë° í‚¤ì›Œë“œ ìƒì„± (ì˜ˆ: LLM ìš”ì•½)\n",
        "    state = analyze_exam(initial_state)\n",
        "\n",
        "    # ë¬¸ì œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "    state = generate_question_strategy(state)\n",
        "\n",
        "    # ë¬¸ì œ ë¦¬ìŠ¤íŠ¸\n",
        "    question_list = state[\"question_strategy\"]\n",
        "\n",
        "    # í•˜ë‚˜ ì„ íƒ\n",
        "    selected_question = random.choice(question_list)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_question\": selected_question,\n",
        "        \"current_strategy\": \"\"  # ì£¼ì œ êµ¬ë¶„ ì—†ìŒ\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "## ---------------- 2ë‹¨ê³„ : ë©´ì ‘ Agent ----------------------\n",
        "\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, Any\n",
        "\n",
        "ExamState = Dict[str, Any]  # ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” íƒ€ì…\n",
        "\n",
        "# ì‚¬ìš©ì ë‹µë³€ì„ ìƒíƒœì— ì €ì¥\n",
        "def update_current_answer(state: ExamState, user_answer: str) -> ExamState:\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_answer\": user_answer.strip()\n",
        "    }\n",
        "\n",
        "# ì •ë‹µ ì—¬ë¶€ë§Œ íŒë‹¨í•˜ëŠ” í‰ê°€ í•¨ìˆ˜\n",
        "def evaluate_answer(state: ExamState) -> ExamState:\n",
        "    current_question = state.get(\"current_question\", {})\n",
        "    current_answer = state.get(\"current_answer\", \"\").strip()\n",
        "\n",
        "    # ì •ë‹µ\n",
        "    correct_answer = current_question.get(\"ì •ë‹µ\", \"\").strip()\n",
        "\n",
        "    # ì‚¬ìš©ì ì…ë ¥ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸\n",
        "    result = {\n",
        "        \"ì •ë‹µì—¬ë¶€\": \"ë§ìŒ\" if current_answer == correct_answer else \"í‹€ë¦¼\",\n",
        "        \"ì •ë‹µ\": correct_answer\n",
        "    }\n",
        "\n",
        "    # ëŒ€í™” ì €ì¥\n",
        "    state[\"conversation\"].append({\n",
        "        \"question\": current_question,\n",
        "        \"answer\": current_answer\n",
        "    })\n",
        "\n",
        "    # í‰ê°€ ì €ì¥\n",
        "    evaluation = state.get(\"evaluation\", [])\n",
        "    result[\"question_index\"] = len(state[\"conversation\"]) - 1\n",
        "    evaluation.append(result)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"evaluation\": evaluation\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# 3) ë¬¸ì œí’€ì´ ì§„í–‰ ê²€í†  --------------------\n",
        "def decide_next_step(state: ExamState) -> ExamState:\n",
        "    conversation = state.get(\"conversation\", [])\n",
        "\n",
        "    # (1) ì´ ì§ˆë¬¸ì´ 20ê°œë¥¼ ì´ˆê³¼í•˜ë©´ ì¢…ë£Œ\n",
        "    if len(conversation) >= 20:\n",
        "        next_step = \"end\"\n",
        "\n",
        "    # (2) ì•„ì§ ë¬¸ì œ ë‚¨ì•„ìˆìœ¼ë©´ ì¶”ê°€ ì§ˆë¬¸\n",
        "    else:\n",
        "        next_step = \"additional_question\"\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "# 4) í•´ì„¤ ìƒì„± --------------------\n",
        "def generate_explanation(state: ExamState) -> ExamState:\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    current_question = state.get(\"current_question\", {})\n",
        "    question_text = current_question.get(\"ë¬¸ì œ\", \"\")\n",
        "    choices = \"\\n\".join(current_question.get(\"ì„ íƒì§€\", []))\n",
        "    correct_answer = current_question.get(\"ì •ë‹µ\", \"\")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ë‹¹ì‹ ì€ 'ì •ì¹˜ì™€ ë²•' ê³¼ëª© ìˆ˜ëŠ¥í˜• ê°ê´€ì‹ ë¬¸ì œì— ëŒ€í•œ í•´ì„¤ì„ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ëŠ” ë¬¸ì œì™€ ì„ íƒì§€, ì •ë‹µì…ë‹ˆë‹¤:\n",
        "- ë¬¸ì œ: {question_text}\n",
        "- ì„ íƒì§€:\n",
        "{choices}\n",
        "- ì •ë‹µ: {correct_answer}\n",
        "\n",
        "ì´ ë¬¸ì œì— ëŒ€í•œ **í•´ì„¤(í’€ì´ ê³¼ì •)**ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
        "í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¬¸ì œë¥¼ ì–´ë–¤ ë²•ì  ê°œë…ê³¼ íŒë¡€ë¥¼ í†µí•´ í•´ê²°í•´ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
        "ë‹µì˜ ê·¼ê±°ë¿ ì•„ë‹ˆë¼, ì˜¤ë‹µì´ ì™œ í‹€ë ¸ëŠ”ì§€ë„ ê°„ëµíˆ ì„¤ëª…í•˜ì„¸ìš”.\n",
        "\n",
        "í˜•ì‹: í•œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, í•™ìƒì´ ê³µë¶€ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
        "\"\"\")\n",
        "\n",
        "    formatted_prompt = prompt.format(\n",
        "        question_text=question_text,\n",
        "        choices=choices,\n",
        "        correct_answer=correct_answer\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    explanation = response.content.strip()\n",
        "\n",
        "    # í•´ì„¤ ì €ì¥\n",
        "    current_question[\"í•´ì„¤\"] = explanation\n",
        "\n",
        "    # ì‚¬ìš©í•œ ë¬¸ì œ ì œì™¸í•˜ê³  ìƒˆë¡œìš´ ë¬¸ì œ ì„ íƒ\n",
        "    used_questions = [turn[\"question\"] for turn in state[\"conversation\"]]\n",
        "    remaining_questions = [q for q in state[\"question_strategy\"] if q not in used_questions]\n",
        "\n",
        "    if remaining_questions:\n",
        "        next_question = random.choice(remaining_questions)\n",
        "    else:\n",
        "        next_question = {\"ë¬¸ì œ\": \"ëª¨ë“  ë¬¸ì œê°€ ì¶œì œë˜ì—ˆìŠµë‹ˆë‹¤.\", \"ì„ íƒì§€\": [], \"ì‚¬ë¡€\": \"\"}\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_question\": next_question,\n",
        "        \"current_answer\": \"\"\n",
        "    }\n",
        "\n",
        "\n",
        "# 5) ë¬¸ì œí’€ì´ í”¼ë“œë°± ë³´ê³ ì„œ --------------------\n",
        "def summarize_exam(state: ExamState) -> ExamState:\n",
        "    summary_blocks = []\n",
        "    print(\"\\n\\U0001F4D8 ë¬¸ì œí’€ì´ ì¢…ë£Œ ë³´ê³ ì„œ: ë¬¸ì œ í’€ì´ ë° ì •ë‹µ í™•ì¸\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for i, turn in enumerate(state[\"conversation\"]):\n",
        "        qnum = i + 1\n",
        "        question = turn[\"question\"].get(\"ë¬¸ì œ\", \"\").strip()\n",
        "        choices = turn[\"question\"].get(\"ì„ íƒì§€\", [])\n",
        "        answer = turn[\"answer\"]\n",
        "        eval_result = state[\"evaluation\"][i] if i < len(state[\"evaluation\"]) else {}\n",
        "        correct = eval_result.get(\"ì •ë‹µ\", \"\")\n",
        "        result = eval_result.get(\"ì •ë‹µì—¬ë¶€\", \"\")\n",
        "        explanation = turn[\"question\"].get(\"í•´ì„¤\", \"í•´ì„¤ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        # ì½˜ì†” ì¶œë ¥\n",
        "        print(f\"[ë¬¸ì œ {qnum}] {question}\")\n",
        "        print(\"[ì„ íƒì§€]\")\n",
        "        for choice in choices:\n",
        "            print(f\"   {choice}\")\n",
        "        print(f\"[ë‹¹ì‹ ì˜ ë‹µë³€] {answer}\")\n",
        "        print(f\"[ì •ë‹µ] {correct}\")\n",
        "        print(f\"[ì •ë‹µ ì—¬ë¶€] {result}\")\n",
        "        print(f\"[í•´ì„¤] {explanation}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # UIìš© ë¸”ë¡ ì¶”ê°€\n",
        "        block = f\"\"\"### ë¬¸ì œ {qnum}\n",
        "**ë¬¸ì œ:** {question}\n",
        "**ë‚´ ë‹µ:** {answer} / **ì •ë‹µ:** {correct} ({result})\n",
        "**í•´ì„¤:** {explanation}\\n\"\"\"\n",
        "        summary_blocks.append(block)\n",
        "\n",
        "        # í•´ì„¤ì´ ì—†ì—ˆë‹¤ë©´ ìƒì„± (ë³´ì™„)\n",
        "        if \"í•´ì„¤\" not in turn[\"question\"]:\n",
        "            state[\"current_question\"] = turn[\"question\"]\n",
        "            state = generate_explanation(state)\n",
        "            turn[\"question\"] = state[\"current_question\"]\n",
        "\n",
        "    # ëª¨ë“  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì„œ ë°˜í™˜ìš© ì¶”ê°€\n",
        "    state[\"final_summary\"] = \"\\n\\n\".join(summary_blocks)\n",
        "    return state\n",
        "\n",
        "# 6) Agent --------------------\n",
        "# ë¶„ê¸° íŒë‹¨ í•¨ìˆ˜\n",
        "def route_next(state: ExamState) -> Literal[\"generate\", \"summarize\"]:\n",
        "    return \"summarize\" if state[\"next_step\"] == \"end\" else \"generate\"\n",
        "\n",
        "# ê·¸ë˜í”„ ì •ì˜ ì‹œì‘\n",
        "builder = StateGraph(ExamState)\n",
        "\n",
        "# ë…¸ë“œ ì¶”ê°€\n",
        "builder.add_node(\"evaluate\", evaluate_answer)\n",
        "builder.add_node(\"decide\", decide_next_step)\n",
        "builder.add_node(\"generate\", generate_explanation)\n",
        "builder.add_node(\"summarize\", summarize_exam)\n",
        "\n",
        "# ë…¸ë“œ ì—°ê²°\n",
        "builder.set_entry_point(\"evaluate\")\n",
        "builder.add_edge(\"evaluate\", \"decide\")\n",
        "builder.add_conditional_edges(\"decide\", route_next)\n",
        "builder.add_edge(\"generate\", END)      # ë£¨í”„\n",
        "builder.add_edge(\"summarize\", END)            # ì¢…ë£Œ\n",
        "\n",
        "# ì»´íŒŒì¼\n",
        "graph = builder.compile()\n",
        "#-------------------------------------------------------------------\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜\n",
        "def initialize_state():\n",
        "    return {\n",
        "        \"state\": None,\n",
        "        \"interview_started\": False,\n",
        "        \"interview_ended\": False,\n",
        "        \"chat_history\": []\n",
        "    }\n",
        "\n",
        "# íŒŒì¼ ì—…ë¡œë“œ í›„ ë¬¸ì œí’€ì´ ì´ˆê¸°í™”\n",
        "def upload_and_initialize(file_obj, session_state):\n",
        "    if file_obj is None:\n",
        "        return session_state, [[\"ğŸ¤– AI\", \"ğŸ“‚ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"]]\n",
        "\n",
        "    file_path = file_obj.name\n",
        "    state = preProcessing_Exam(file_path)\n",
        "\n",
        "    session_state[\"state\"] = state\n",
        "    session_state[\"interview_started\"] = True\n",
        "\n",
        "    current = state[\"current_question\"]\n",
        "    question_text = current.get(\"ë¬¸ì œ\", \"\")\n",
        "    case_text = current.get(\"ì‚¬ë¡€\", \"\")\n",
        "    choices_text = \"\\n\".join(current.get(\"ì„ íƒì§€\", []))\n",
        "    full_question = f\"{case_text}\\n\\n{question_text}\\n{choices_text}\"\n",
        "\n",
        "    chat_history = [\n",
        "        [\"ğŸ¤– AI\", \"ë¬¸ì œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\"],\n",
        "        [\"ğŸ¤– AI ë¬¸ì œ\", full_question]\n",
        "    ]\n",
        "    session_state[\"chat_history\"] = chat_history\n",
        "    full_question = f\"[ë¬¸ì œ {current.get('ë¬¸ì œë²ˆí˜¸', '?')}] {case_text}\\n\\n{question_text}\\n{choices_text}\"\n",
        "\n",
        "    return session_state, chat_history\n",
        "\n",
        "# ë‹µë³€ ì²˜ë¦¬ ë° ë‹¤ìŒ ë¬¸ì œ ìƒì„±\n",
        "def chat_interview(user_input, session_state):\n",
        "    if not session_state[\"interview_started\"]:\n",
        "        return session_state, [[\"ğŸ¤– AI\", \"ë¨¼ì € ê¸°ì¶œë¬¸ì œë¥¼ ì—…ë¡œë“œí•˜ê³  ì‹œì‘í•˜ì„¸ìš”.\"]]\n",
        "\n",
        "    # ì‚¬ìš©ì ë‹µë³€ ì €ì¥\n",
        "    session_state[\"chat_history\"].append([\"ğŸ™‹â€â™‚ï¸ ì‚¬ìš©ì\", user_input])\n",
        "    session_state[\"state\"] = update_current_answer(session_state[\"state\"], user_input)\n",
        "\n",
        "    # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "    session_state[\"state\"] = graph.invoke(session_state[\"state\"])\n",
        "\n",
        "    if session_state[\"state\"][\"next_step\"] == \"end\":\n",
        "        session_state[\"interview_ended\"] = True\n",
        "        session_state[\"state\"] = summarize_exam(session_state[\"state\"])\n",
        "\n",
        "        summary_text = session_state[\"state\"].get(\"final_summary\", \"ê²°ê³¼ ìš”ì•½ ì—†ìŒ\")\n",
        "        session_state[\"chat_history\"].append([\"ğŸ¤– AI ë¬¸ì œ\", \"âœ… ë¬¸ì œí’€ì´ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\n\" + summary_text])\n",
        "\n",
        "        return session_state, session_state[\"chat_history\"]\n",
        "    else:\n",
        "        current = session_state[\"state\"][\"current_question\"]\n",
        "        question_text = current.get(\"ë¬¸ì œ\", \"\")\n",
        "        case_text = current.get(\"ì‚¬ë¡€\", \"\")\n",
        "        choices_text = \"\\n\".join(current.get(\"ì„ íƒì§€\", []))\n",
        "        qnum = current.get(\"ë¬¸ì œë²ˆí˜¸\", \"?\")\n",
        "        full_question = f\"[ë¬¸ì œ {qnum}] {case_text}\\n\\n{question_text}\\n{choices_text}\"\n",
        "\n",
        "        session_state[\"chat_history\"].append([\"ğŸ¤– AI ë¬¸ì œ\", full_question])\n",
        "        return session_state, session_state[\"chat_history\"]\n",
        "\n",
        "\n",
        "# Gradio UI êµ¬ì„±\n",
        "with gr.Blocks() as demo:\n",
        "    session_state = gr.State(initialize_state())\n",
        "\n",
        "    gr.Markdown(\"# ğŸ§  AI ë¬¸ì œí’€ì´ ì‹œìŠ¤í…œ\\nê¸°ì¶œë¬¸ì œë¥¼ ì—…ë¡œë“œí•˜ê³  ìˆ˜ëŠ¥í˜• ë¬¸ì œë¥¼ í’€ì–´ë³´ì„¸ìš”!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"ê¸°ì¶œë¬¸ì œ ì—…ë¡œë“œ (PDF ë˜ëŠ” DOCX)\")\n",
        "        upload_btn = gr.Button(\"ë¬¸ì œí’€ì´ ì‹œì‘\")\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    user_input = gr.Textbox(show_label=False, placeholder=\"ë‹µì„ â‘ , â‘¡, â‘¢ ë“±ì˜ í˜•ì‹ìœ¼ë¡œ ì…ë ¥ í›„ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.\")\n",
        "\n",
        "    upload_btn.click(upload_and_initialize, inputs=[file_input, session_state], outputs=[session_state, chatbot])\n",
        "    user_input.submit(chat_interview, inputs=[user_input, session_state], outputs=[session_state, chatbot])\n",
        "    user_input.submit(lambda: \"\", None, user_input)\n",
        "\n",
        "# ì‹¤í–‰\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvYpBoXHZfvd",
        "outputId": "55f84ebd-af24-4fc0-f6fc-858e0c4867bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. ì‹¤í–‰**"
      ],
      "metadata": {
        "id": "vVw8pSQRyy73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "JRnTUFUs_1f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da12e44-d7c8-4e69-c9eb-84cf9025c6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/app.py:485: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n",
            "* Running on local URL:  http://127.0.0.1:7865\n",
            "* Running on public URL: https://0aa6fa9fd96dd72236.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3075, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 493, in <module>\n",
            "    demo.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2981, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3079, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1123, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7865 <> https://0aa6fa9fd96dd72236.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MTnQq6qKWLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}